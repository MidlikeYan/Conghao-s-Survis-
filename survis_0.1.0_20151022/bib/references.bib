@InProceedings{Paper1, Pu_2023_CVPR,
    author    = {Pu, Nan and Zhong, Zhun and Sebe, Nicu},
    title     = {Dynamic Conceptional Contrastive Learning for Generalized Category Discovery},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    keywords = {type:Generalized Category Discovery},
    pages     = {7579-7588}

}

@InProceedings{Paper2, Vaze_2022_CVPR,
    author    = {Vaze, Sagar and Han, Kai and Vedaldi, Andrea and Zisserman, Andrew},
    title     = {Generalized Category Discovery},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    keywords = {type:Generalized Category Discovery},
    pages     = {7492-7501}
}
@InProceedings{Paper3, Wen_2023_ICCV,
    author    = {Wen, Xin and Zhao, Bingchen and Qi, Xiaojuan},
    title     = {Parametric Classification for Generalized Category Discovery: A Baseline Study},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    keywords = {type:Generalized Category Discovery},
    pages     = {16590-16600}
}
@misc{Paper4, wang2024sptnet,
      title={SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning},
      author={Hongjun Wang and Sagar Vaze and Kai Han},
      year={2024},
      eprint={2403.13684},
      archivePrefix={arXiv},
      keywords = {type:Generalized Category Discovery},
      primaryClass={cs.CV}
}
@misc{Paper5, choi2024contrastive,
      title={Contrastive Mean-Shift Learning for Generalized Category Discovery},
      author={Sua Choi and Dahyun Kang and Minsu Cho},
      year={2024},
      eprint={2404.09451},
      archivePrefix={arXiv},
      keywords = {type:Generalized Category Discovery},
      primaryClass={cs.CV}
}
@inproceedings{Paper6, NEURIPS2019_1cd138d0,
 author = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {MixMatch: A Holistic Approach to Semi-Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf},
 volume = {32},
 keywords = {type:Semi-supervised Learning},
 year = {2019}
}
@InProceedings{Paper7, pmlr-v119-chen20j,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  keywords = {type:Contrastive Learning},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}
@ARTICLE{Paper8, 9464163,
  author={Han, Kai and Rebuffi, Sylvestre-Alvise and Ehrhardt, Sébastien and Vedaldi, Andrea and Zisserman, Andrew},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={AutoNovel: Automatically Discovering and Learning Novel Visual Categories},
  year={2022},
  volume={44},
  number={10},
  pages={6767-6781},
  keywords={type:Novel Categoriy Discovery},
  doi={10.1109/TPAMI.2021.3091944}}
@InProceedings{Paper9, Fini_2021_ICCV,
    author    = {Fini, Enrico and Sangineto, Enver and Lathuili\`ere, St\'ephane and Zhong, Zhun and Nabi, Moin and Ricci, Elisa},
    title     = {A Unified Objective for Novel Class Discovery},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    keywords={type:Novel Categoriy Discovery},
    pages     = {9284-9292}
}

@article{sPaper10, https://doi.org/10.1002/wics.1270,
author = {Bair, Eric},
title = {Semi-supervised clustering methods},
journal = {WIREs Computational Statistics},
volume = {5},
number = {5},
pages = {349-361},
keywords = {type:Semi-supervised Learning},
doi = {https://doi.org/10.1002/wics.1270},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1270},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1270},
abstract = {Cluster analysis methods seek to partition a data set into homogeneous subgroups. It is useful in a wide variety of applications, including document processing and modern genetics. Conventional clustering methods are unsupervised, meaning that there is no outcome variable nor is anything known about the relationship between the observations in the data set. In many situations, however, information about the clusters is available in addition to the values of the features. For example, the cluster labels of some observations may be known, or certain observations may be known to belong to the same cluster. In other cases, one may wish to identify clusters that are associated with a particular outcome variable. This review describes several clustering algorithms (known as ‘semi-supervised clustering’ methods) that can be applied in these situations. The majority of these methods are modifications of the popular k-means clustering method, and several of them will be described in detail. A brief description of some other semi-supervised clustering algorithms is also provided. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences > Clustering and Classification},
year = {2013}
}

